{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Remove duplicate PMids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('data_all.xlsx', sheet_name='article')  \n",
    "GMDB = pd.read_csv('image_metadata_v1.1.0.tsv', sep='\\t')\n",
    "GMDB = GMDB.dropna(subset=['pmid'])\n",
    "GMDB['pmid'] = pd.to_numeric(GMDB['pmid'], errors='coerce').astype('Int64')  # 注意大写的Int64\n",
    "df['pmid'] = pd.to_numeric(df['pmid'], errors='coerce').astype('Int64')\n",
    "\n",
    "merged = GMDB.merge(df[['pmid']], on='pmid', how='left', indicator=True)\n",
    "GMDB_unique = merged[merged['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "\n",
    "GMDB_unique = GMDB[~GMDB['pmid'].isin(df['pmid']) & GMDB['pmid'].notna()]\n",
    "print(f\"GMDB原始记录数: {len(GMDB)}\")\n",
    "print(f\"唯一记录数: {len(GMDB_unique)}\")\n",
    "GMDB_unique.to_csv(\"GMDB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bind the OMIM id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data_all.xlsx', sheet_name='article')  \n",
    "GMDB = pd.read_csv('GMDB.csv')\n",
    "GMDB_syndrome = pd.read_csv('gmdb_syndromes_v1.1.0.tsv', sep='\\t')\n",
    "GMDB['OMIM'] = GMDB['internal_syndrome_name'].map(GMDB_syndrome.set_index('syndrome_name')['OMIM'])\n",
    "\n",
    "GMDB = GMDB.dropna(subset=['OMIM'])\n",
    "GMDB.to_csv(\"GMDB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article node\n",
    "from Bio import Entrez\n",
    "from Bio.Entrez import efetch, read\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "article_node = GMDB[[\"pmid\", \"author\"]]\n",
    "article_node = article_node.drop_duplicates()\n",
    "\n",
    "def fetch_pubmed_details(pmids, batch_size=100):\n",
    "    all_details = []\n",
    "    # 将 PMID 分批次处理\n",
    "    for i in tqdm(range(0, len(pmids), batch_size)):\n",
    "        batch = pmids[i:i+batch_size]\n",
    "        try:\n",
    "            handle = efetch(db=\"pubmed\", id=batch, retmode=\"xml\")\n",
    "            records = read(handle)[\"PubmedArticle\"]\n",
    "            for record in records:\n",
    "                article = record[\"MedlineCitation\"][\"Article\"]\n",
    "                pmid = record[\"MedlineCitation\"][\"PMID\"]\n",
    "                title = article.get(\"ArticleTitle\", \"N/A\")\n",
    "                journal_info = article.get(\"Journal\", {})\n",
    "                journal_name = journal_info.get(\"Title\", \"N/A\")\n",
    "                all_details.append({\n",
    "                    \"PMID\": pmid,\n",
    "                    \"Title\": title,\n",
    "                    \"magazineName\": journal_name,\n",
    "                })\n",
    "                \n",
    "            # 遵守 NCBI 的请求频率限制（每秒1次）\n",
    "            time.sleep(1)  # 无 API Key 时需更严格限制\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching PMIDs {batch}: {str(e)}\")\n",
    "            continue\n",
    "    return all_details\n",
    "\n",
    "pmids = article_node[\"pmid\"].tolist()\n",
    "pubmed_data = fetch_pubmed_details(pmids)\n",
    "pubmed_data_df = pd.DataFrame(pubmed_data)\n",
    "pubmed_data_df['PMID'] = pd.to_numeric(pubmed_data_df['PMID'], errors='coerce').astype('Int64')\n",
    "article_node = pd.merge(\n",
    "    article_node,\n",
    "    pubmed_data_df,\n",
    "    left_on=\"pmid\",\n",
    "    right_on=\"PMID\",\n",
    "    how=\"left\"\n",
    ").drop(\"PMID\", axis=1)\n",
    "\n",
    "article_node = article_node.rename(columns={\"Title\": \"articleName\",})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disease node\n",
    "disease_node = GMDB[[\"OMIM\", \"internal_syndrome_name\"]]\n",
    "disease_node = disease_node.drop_duplicates()\n",
    "disease_node.columns = [\"did\", \"dname\"]\n",
    "disease_node['did'] = pd.to_numeric(disease_node['did'], errors='coerce').astype('Int64')\n",
    "    # merge\n",
    "df_disease = pd.read_excel('data_all.xlsx', sheet_name='diseases')\n",
    "df_disease = df_disease.rename(columns={\"Did\": \"did\",})\n",
    "merged = disease_node.merge(df_disease[['did']], on='did', how='left', indicator=True)\n",
    "disease_node = merged[merged['_merge'] == 'left_only'].drop('_merge', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15085\\AppData\\Local\\Temp\\ipykernel_26532\\2014670767.py:16: UnicodeWarning: unsound encoding, assuming ISO-8859-1 (73% confidence)\n",
      "  hpo = pronto.Ontology(\"hp.obo\")\n"
     ]
    }
   ],
   "source": [
    "# phenotype node\n",
    "import pronto\n",
    "phenotype_node = GMDB[[\"present_features\"]]\n",
    "phenotype_node = phenotype_node.rename(columns={\"present_features\": \"pid\",})\n",
    "phenotype_node = phenotype_node.dropna(subset=['pid'])\n",
    "    # expand\n",
    "phenotype_node[\"pid\"] = phenotype_node[\"pid\"].str.split(\";\").apply(lambda x: [s.strip() for s in x])\n",
    "phenotype_node = phenotype_node.explode(\"pid\").reset_index(drop=True)\n",
    "phenotype_node = phenotype_node.drop_duplicates()\n",
    "    # merge\n",
    "df_phenotype = pd.read_excel('data_all.xlsx', sheet_name='phenotype')\n",
    "df_phenotype = df_phenotype.rename(columns={\"Pid\": \"pid\",})\n",
    "merged = phenotype_node.merge(df_phenotype[['pid']], on='pid', how='left', indicator=True)\n",
    "phenotype_node = merged[merged['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "    # mapping name\n",
    "hpo = pronto.Ontology(\"hp.obo\")\n",
    "def get_hpo_term(hpo_id):\n",
    "    term = hpo.get(hpo_id)\n",
    "    return term.name if term else \"Unknown\"\n",
    "phenotype_node[\"phenotypeName\"] = phenotype_node[\"pid\"].apply(get_hpo_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genotype node\n",
    "genotype_node = GMDB[[\"gene_names\",\"gene_entrez_ids\"]]\n",
    "genotype_node = genotype_node.rename(columns={\"gene_names\": \"geneName\",  \"gene_entrez_ids\": \"geneId\"})\n",
    "genotype_node = genotype_node.dropna(subset=['geneName'])\n",
    "    # expand\n",
    "genotype_node[\"geneName\"] = genotype_node[\"geneName\"].str.split(r\",\\s*\")  # 按逗号分割，允许空格\n",
    "genotype_node[\"geneId\"] = genotype_node[\"geneId\"].str.split(r\",\\s*\")\n",
    "genotype_node = genotype_node.explode([\"geneName\", \"geneId\"]).reset_index(drop=True)\n",
    "genotype_node = genotype_node.drop_duplicates()\n",
    "genotype_node[\"geneId\"] = pd.to_numeric(genotype_node[\"geneId\"]).astype('Int64')\n",
    "    # merge\n",
    "df_genotype = pd.read_excel('data_all.xlsx', sheet_name='genotype')\n",
    "df_genotype = df_genotype.rename(columns={\"gene_id\": \"geneId\",})\n",
    "df_genotype[\"geneId\"] = pd.to_numeric(df_genotype[\"geneId\"], errors='coerce').astype('Int64')\n",
    "merged = genotype_node.merge(df_genotype[['geneId']], on='geneId', how='left', indicator=True)\n",
    "genotype_node = merged[merged['_merge'] == 'left_only'].drop('_merge', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variation node (details, category)\n",
    "from ast import literal_eval\n",
    "variation_node = GMDB[[\"hgvs\"]]\n",
    "variation_node = variation_node.rename(columns={\"hgvs\": \"details\",})\n",
    "variation_node[\"details\"] = variation_node[\"details\"].apply(lambda x: literal_eval(x) if isinstance(x, str) else x)\n",
    "variation_node = variation_node[variation_node[\"details\"].apply(len) > 0]\n",
    "variation_node[\"details\"] = variation_node[\"details\"].apply(lambda x: \", \".join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "variation_node = variation_node.drop_duplicates()\n",
    "    # merge\n",
    "df_variation = pd.read_excel('data_all.xlsx', sheet_name='relation_gd')\n",
    "merged = variation_node.merge(df_variation[['details']], on='details', how='left', indicator=True)\n",
    "variation_node = merged[merged['_merge'] == 'left_only'].drop('_merge', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample node (sid, gender, race, samplenumber, age)\n",
    "sample_node = GMDB[[\"pmid\", \"gender\", \"ethnicity_category\", \"age_year\", \"age_month\"]]\n",
    "sample_node = sample_node.rename(columns={\"pmid\": \"sid\", \"ethnicity_category\":\"race\", })\n",
    "sample_node[\"sid\"] = \"s_\" + sample_node[\"sid\"].astype(str) + \"_\" + (sample_node.groupby(\"sid\").cumcount() + 1).astype(str)\n",
    "sample_node[\"gender\"] = sample_node[\"gender\"].replace({\"female\": \"F\", \"male\": \"M\"})\n",
    "sample_node[\"age\"] = sample_node[\"age_year\"] + sample_node[\"age_month\"] / 12\n",
    "sample_node = sample_node.drop(columns=[\"age_year\", \"age_month\"])\n",
    "sample_node = sample_node.rename(columns={\"age\": \"year\", })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_node.to_csv(\"./nodes/article_node.csv\", index=False)\n",
    "disease_node.to_csv(\"./nodes/disease_node.csv\", index=False)\n",
    "phenotype_node.to_csv(\"./nodes/phenotype_node.csv\", index=False)\n",
    "genotype_node.to_csv(\"./nodes/genotype_node.csv\", index=False)\n",
    "variation_node.to_csv(\"./nodes/variation_node.csv\", index=False)\n",
    "sample_node.to_csv(\"./nodes/sample_node.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyphers to import nodes\n",
    "LOAD CSV WITH HEADERS FROM 'file:///article_node.csv' AS row CREATE (:Article { pmid: toInteger(row.pmid), author: row.author, articleName: row.articleName, magazineName: row.magazineName }); \n",
    "LOAD CSV WITH HEADERS FROM 'file:///disease_node.csv' AS row CREATE (:Disease { did: toInteger(row.did), dname: row.dname }); \n",
    "LOAD CSV WITH HEADERS FROM 'file:///phenotype_node.csv' AS row CREATE (:FacePhenotype { pid: row.pid, phenotypeName: row.phenotypeName }); \n",
    "LOAD CSV WITH HEADERS FROM 'file:///genotype_node.csv' AS row CREATE (:Genotype { geneName: row.geneName, geneId: toInteger(row.geneId) }); \n",
    "LOAD CSV WITH HEADERS FROM 'file:///variation_node.csv' AS row CREATE (:Variation { details: row.details}); \n",
    "LOAD CSV WITH HEADERS FROM 'file:///sample_node.csv' AS row CREATE (:Sample {sid: row.sid, gender: row.gender, race: row.race, year: row.year }); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation_sample_phenotype   (sid, pid)\n",
    "relation_sample_phenotype = GMDB[[\"present_features\", \"pmid\"]]\n",
    "relation_sample_phenotype = relation_sample_phenotype.rename(columns={\"pmid\": \"sid\", \"present_features\":\"pid\", })\n",
    "relation_sample_phenotype[\"sid\"] = \"s_\" + relation_sample_phenotype[\"sid\"].astype(str) + \"_\" + (relation_sample_phenotype.groupby(\"sid\").cumcount() + 1).astype(str)\n",
    "relation_sample_phenotype = relation_sample_phenotype.dropna(subset=['pid'])\n",
    "    # expand\n",
    "relation_sample_phenotype[\"pid\"] = relation_sample_phenotype[\"pid\"].str.split(\";\").apply(lambda x: [s.strip() for s in x])\n",
    "relation_sample_phenotype = relation_sample_phenotype.explode(\"pid\").reset_index(drop=True)\n",
    "relation_sample_phenotype = relation_sample_phenotype.drop_duplicates()\n",
    "relation_sample_phenotype['type'] = \"Mention_FP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation_sample_variation  (sid, details)\n",
    "relation_sample_variation = GMDB[[\"pmid\", \"hgvs\"]]\n",
    "relation_sample_variation = relation_sample_variation.rename(columns={\"pmid\": \"sid\", \"hgvs\": \"details\",})\n",
    "relation_sample_variation[\"sid\"] = \"s_\" + relation_sample_variation[\"sid\"].astype(str) + \"_\" + (relation_sample_variation.groupby(\"sid\").cumcount() + 1).astype(str)\n",
    "relation_sample_variation[\"details\"] = relation_sample_variation[\"details\"].apply(lambda x: literal_eval(x) if isinstance(x, str) else x)\n",
    "relation_sample_variation = relation_sample_variation[relation_sample_variation[\"details\"].apply(len) > 0]\n",
    "\n",
    "relation_sample_variation[\"details\"] = relation_sample_variation[\"details\"].apply(lambda x: \", \".join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "relation_sample_variation = relation_sample_variation.drop_duplicates()\n",
    "relation_sample_variation['type'] = \"Mention_Var\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation_variation_gene (details, gid)\n",
    "relation_variation_gene = GMDB[[\"hgvs\", \"gene_entrez_ids\"]]\n",
    "relation_variation_gene = relation_variation_gene.rename(columns={\"hgvs\": \"details\", \"gene_entrez_ids\": \"geneId\",})\n",
    "relation_variation_gene = relation_variation_gene.dropna(subset=['geneId'])\n",
    "relation_variation_gene[\"details\"] = relation_variation_gene[\"details\"].apply(lambda x: literal_eval(x) if isinstance(x, str) else x)\n",
    "relation_variation_gene = relation_variation_gene[relation_variation_gene[\"details\"].apply(len) > 0]\n",
    "relation_variation_gene[\"details\"] = relation_variation_gene[\"details\"].apply(lambda x: \", \".join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "    # expand\n",
    "relation_variation_gene[\"geneId\"] = relation_variation_gene[\"geneId\"].str.split(r\",\\s*\")\n",
    "relation_variation_gene = relation_variation_gene.explode([\"geneId\"]).reset_index(drop=True)\n",
    "relation_variation_gene = relation_variation_gene.drop_duplicates()\n",
    "relation_variation_gene[\"geneId\"] = pd.to_numeric(relation_variation_gene[\"geneId\"]).astype('Int64')\n",
    "relation_variation_gene['type'] = \"Have\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation_variation_disease  (details, did)\n",
    "relation_variation_disease = GMDB[[\"hgvs\", \"OMIM\"]]\n",
    "relation_variation_disease = relation_variation_disease.rename(columns={\"hgvs\": \"details\", \"OMIM\": \"did\",})\n",
    "relation_variation_disease[\"details\"] = relation_variation_disease[\"details\"].apply(lambda x: literal_eval(x) if isinstance(x, str) else x)\n",
    "relation_variation_disease = relation_variation_disease[relation_variation_disease[\"details\"].apply(len) > 0]\n",
    "relation_variation_disease[\"details\"] = relation_variation_disease[\"details\"].apply(lambda x: \", \".join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "relation_variation_disease['did'] = pd.to_numeric(relation_variation_disease['did'], errors='coerce').astype('Int64')\n",
    "relation_variation_disease = relation_variation_disease.drop_duplicates()\n",
    "relation_variation_disease['type'] = \"Cause\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation_gene_phenotype  (gid, pid)\n",
    "relation_gene_phenotype = GMDB[[\"gene_entrez_ids\", \"present_features\"]]\n",
    "relation_gene_phenotype = relation_gene_phenotype.rename(columns={\"gene_entrez_ids\": \"geneId\", \"present_features\":\"pid\", })\n",
    "relation_gene_phenotype = relation_gene_phenotype.dropna(subset=['geneId', 'pid'])\n",
    "    # expand\n",
    "relation_gene_phenotype[\"geneId\"] = relation_gene_phenotype[\"geneId\"].str.split(r\",\\s*\")\n",
    "relation_gene_phenotype[\"pid\"] = relation_gene_phenotype[\"pid\"].str.split(\";\").apply(lambda x: [s.strip() for s in x])\n",
    "relation_gene_phenotype = relation_gene_phenotype.explode([\"pid\"]).reset_index(drop=True)\n",
    "relation_gene_phenotype = relation_gene_phenotype.explode([\"geneId\"]).reset_index(drop=True)\n",
    "relation_gene_phenotype = relation_gene_phenotype.drop_duplicates()\n",
    "relation_gene_phenotype[\"geneId\"] = pd.to_numeric(relation_gene_phenotype[\"geneId\"]).astype('Int64')\n",
    "relation_gene_phenotype['type'] = \"Affect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation_disease_phenotype (did, pid)\n",
    "relation_disease_phenotype = GMDB[[\"OMIM\", \"present_features\"]]\n",
    "relation_disease_phenotype = relation_disease_phenotype.rename(columns={\"OMIM\": \"did\", \"present_features\":\"pid\", })\n",
    "relation_disease_phenotype = relation_disease_phenotype.dropna(subset=['did', 'pid'])\n",
    "    # expand\n",
    "relation_disease_phenotype[\"pid\"] = relation_disease_phenotype[\"pid\"].str.split(\";\").apply(lambda x: [s.strip() for s in x])\n",
    "relation_disease_phenotype = relation_disease_phenotype.explode([\"pid\"]).reset_index(drop=True)\n",
    "relation_disease_phenotype = relation_disease_phenotype.drop_duplicates()\n",
    "relation_disease_phenotype['did'] = pd.to_numeric(relation_disease_phenotype['did'], errors='coerce').astype('Int64')\n",
    "relation_disease_phenotype['type'] = \"Has_Phenotype\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15085\\AppData\\Local\\Temp\\ipykernel_26532\\3799964966.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  relation_sample_article['sid'] = relation_sample_article['pmid']\n",
      "C:\\Users\\15085\\AppData\\Local\\Temp\\ipykernel_26532\\3799964966.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  relation_sample_article[\"sid\"] = \"s_\" + relation_sample_article[\"sid\"].astype(str) + \"_\" + (relation_sample_article.groupby(\"sid\").cumcount() + 1).astype(str)\n"
     ]
    }
   ],
   "source": [
    "# relation_sample_article (sid, pmid)\n",
    "relation_sample_article = GMDB[[\"pmid\"]]\n",
    "relation_sample_article['sid'] = relation_sample_article['pmid']\n",
    "relation_sample_article[\"sid\"] = \"s_\" + relation_sample_article[\"sid\"].astype(str) + \"_\" + (relation_sample_article.groupby(\"sid\").cumcount() + 1).astype(str)\n",
    "relation_sample_article = relation_sample_article.drop_duplicates()\n",
    "relation_sample_article['type'] = \"Exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation_sample_disease_origin (sid, did) diagnosed_with\n",
    "FGDD = pd.read_csv(\"FGDD.csv\")\n",
    "relation_sample_disease_origin = FGDD[[\"patient_id\", \"Disease_id\"]]\n",
    "relation_sample_disease_origin = relation_sample_disease_origin.rename(columns={\"patient_id\": \"sid\", \"Disease_id\":\"did\", })\n",
    "relation_sample_disease_origin = relation_sample_disease_origin.dropna(subset=['sid', 'did'])\n",
    "relation_sample_disease_origin['did'] = pd.to_numeric(relation_sample_disease_origin['did'], errors='coerce').astype('Int64')\n",
    "relation_sample_disease_origin['type'] = \"Diagnosed_With\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation_sample_disease (sid, did) diagnosed_with\n",
    "relation_sample_disease = GMDB[[\"pmid\", \"OMIM\"]]\n",
    "relation_sample_disease = relation_sample_disease.rename(columns={\"pmid\": \"sid\", \"OMIM\":\"did\", })\n",
    "relation_sample_disease[\"sid\"] = \"s_\" + relation_sample_disease[\"sid\"].astype(str) + \"_\" + (relation_sample_disease.groupby(\"sid\").cumcount() + 1).astype(str)\n",
    "relation_sample_disease = relation_sample_disease.drop_duplicates(subset=['sid', 'did'])\n",
    "relation_sample_disease['did'] = pd.to_numeric(relation_sample_disease['did'], errors='coerce').astype('Int64')\n",
    "relation_sample_disease['type'] = \"Diagnosed_With\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_sample_phenotype.to_csv(\"./relationships/relation_sample_phenotype.csv\" ,index=False)\n",
    "relation_sample_variation.to_csv(\"./relationships/relation_sample_variation.csv\" ,index=False)\n",
    "relation_variation_gene.to_csv(\"./relationships/relation_variation_gene.csv\" ,index=False)\n",
    "relation_variation_disease.to_csv(\"./relationships/relation_variation_disease.csv\" ,index=False)\n",
    "relation_gene_phenotype.to_csv(\"./relationships/relation_gene_phenotype.csv\" ,index=False)\n",
    "relation_disease_phenotype.to_csv(\"./relationships/relation_disease_phenotype.csv\" ,index=False)\n",
    "relation_sample_article.to_csv(\"./relationships/relation_sample_article.csv\" ,index=False)\n",
    "\n",
    "relation_sample_disease_origin.to_csv(\"./relationships/relation_sample_disease_origin.csv\" ,index=False)\n",
    "relation_sample_disease.to_csv(\"./relationships/relation_sample_disease.csv\" ,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cypher to import relationships\n",
    "LOAD CSV WITH HEADERS FROM 'file:///relation_sample_phenotype.csv' AS row MATCH (n1:FacePhenotype {pid: row.pid}) MATCH (n2:Sample {sid: row.sid}) CREATE (n1)-[r: Mention_FP]->(n2) RETURN count(r);\n",
    "LOAD CSV WITH HEADERS FROM 'file:///relation_sample_variation.csv' AS row MATCH (n1:Sample {sid: row.sid}) MATCH (n2:Variation {details: row.details}) CREATE (n1)-[r: Mention_Var]->(n2) RETURN count(r);\n",
    "LOAD CSV WITH HEADERS FROM 'file:///relation_sample_article.csv' AS row MATCH (n1:Sample {sid: row.sid}) MATCH (n2:Article {pmid: toInteger(row.pmid)}) CREATE (n1)-[r: Exist]->(n2) RETURN count(r);\n",
    "\n",
    "LOAD CSV WITH HEADERS FROM 'file:///relation_variation_gene.csv' AS row MATCH (n1:Variation {details: row.details}) MATCH (n2:Genotype {geneId: toInteger(row.geneId)}) CREATE (n1)-[r: Have]->(n2) RETURN count(r);\n",
    "LOAD CSV WITH HEADERS FROM 'file:///relation_variation_disease.csv' AS row MATCH (n1:Variation {details: row.details}) MATCH (n2:Disease {did: toInteger(row.did)}) CREATE (n1)-[r: Cause]->(n2) RETURN count(r);\n",
    "\n",
    "LOAD CSV WITH HEADERS FROM 'file:///relation_gene_phenotype.csv' AS row MATCH (n1:Genotype {geneId: toInteger(row.geneId)}) MATCH (n2:FacePhenotype {pid: row.pid}) CREATE (n1)-[r: Affect]->(n2) RETURN count(r);\n",
    "LOAD CSV WITH HEADERS FROM 'file:///relation_disease_phenotype.csv' AS row MATCH (n1:Disease {did: toInteger(row.did)}) MATCH (n2:FacePhenotype {pid: row.pid}) CREATE (n1)-[r: Has_Phenotype]->(n2) RETURN count(r);\n",
    "\n",
    "\n",
    "\n",
    "LOAD CSV WITH HEADERS FROM 'file:///relation_sample_disease_origin.csv' AS row MATCH (n1:Sample {sid: row.sid}) MATCH (n2:Disease {did: toInteger(row.did)}) CREATE (n1)-[r: Diagnosed_With]->(n2) RETURN count(r);\n",
    "LOAD CSV WITH HEADERS FROM 'file:///relation_sample_disease.csv' AS row MATCH (n1:Sample {sid: row.sid}) MATCH (n2:Disease {did: toInteger(row.did)}) CREATE (n1)-[r: Diagnosed_With]->(n2) RETURN count(r);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
